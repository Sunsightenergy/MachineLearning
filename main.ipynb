{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensorflow-2.13.0-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading h5py-3.11.0-cp38-cp38-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading protobuf-4.25.5-cp38-cp38-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.66.1)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.34.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.32.3)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (8.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.20.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\chase\\.conda\\envs\\cuda_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.6.1)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.13.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
      "Downloading tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl (276.5 MB)\n",
      "   ---------------------------------------- 0.0/276.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 5.8/276.5 MB 32.0 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 11.8/276.5 MB 30.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 18.4/276.5 MB 31.2 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 24.1/276.5 MB 31.1 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 29.4/276.5 MB 29.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 35.7/276.5 MB 30.2 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 43.0/276.5 MB 31.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 48.5/276.5 MB 30.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 54.5/276.5 MB 30.5 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 62.4/276.5 MB 31.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 68.2/276.5 MB 31.0 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 75.5/276.5 MB 31.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 79.7/276.5 MB 30.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 85.2/276.5 MB 30.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 87.0/276.5 MB 29.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 88.3/276.5 MB 27.4 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 95.7/276.5 MB 27.9 MB/s eta 0:00:07\n",
      "   -------------- ------------------------ 101.7/276.5 MB 28.0 MB/s eta 0:00:07\n",
      "   --------------- ----------------------- 108.8/276.5 MB 28.3 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 114.0/276.5 MB 28.2 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 118.8/276.5 MB 27.9 MB/s eta 0:00:06\n",
      "   ----------------- --------------------- 125.8/276.5 MB 28.3 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 133.2/276.5 MB 28.8 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 137.6/276.5 MB 28.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 143.7/276.5 MB 28.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 145.8/276.5 MB 27.7 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 151.8/276.5 MB 27.7 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 157.3/276.5 MB 27.8 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 164.1/276.5 MB 28.0 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 170.1/276.5 MB 27.9 MB/s eta 0:00:04\n",
      "   ------------------------ -------------- 176.2/276.5 MB 28.0 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 182.7/276.5 MB 28.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 188.5/276.5 MB 28.1 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 195.8/276.5 MB 28.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 202.4/276.5 MB 28.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 207.6/276.5 MB 28.4 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 213.9/276.5 MB 28.6 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 218.1/276.5 MB 28.6 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 221.2/276.5 MB 28.1 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 226.8/276.5 MB 27.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 232.8/276.5 MB 28.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 239.1/276.5 MB 28.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 245.1/276.5 MB 28.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 250.6/276.5 MB 28.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 257.2/276.5 MB 28.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 263.2/276.5 MB 28.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  269.5/276.5 MB 28.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  275.0/276.5 MB 28.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.3/276.5 MB 28.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.3/276.5 MB 28.0 MB/s eta 0:00:01\n",
      "   --------------------------------------- 276.5/276.5 MB 26.6 MB/s eta 0:00:00\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.11.0-cp38-cp38-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 44.0 MB/s eta 0:00:00\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 96.1 MB/s eta 0:00:00\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.5-cp38-cp38-win_amd64.whl (413 kB)\n",
      "Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.6/5.6 MB 48.3 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 38.3 MB/s eta 0:00:00\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Downloading wrapt-1.16.0-cp38-cp38-win_amd64.whl (37 kB)\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, protobuf, opt-einsum, oauthlib, keras, h5py, google-pasta, gast, astunparse, absl-py, requests-oauthlib, markdown, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.1\n",
      "    Uninstalling protobuf-5.28.1:\n",
      "      Successfully uninstalled protobuf-5.28.1\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.4.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 h5py-3.11.0 keras-2.13.1 libclang-18.1.1 markdown-3.7 oauthlib-3.2.2 opt-einsum-3.4.0 protobuf-4.25.5 requests-oauthlib-2.0.0 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 typing-extensions-4.5.0 werkzeug-3.0.6 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.4.0 requires fsspec, which is not installed.\n",
      "grpcio-status 1.66.1 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.5 which is incompatible.\n",
      "torch 2.4.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/data_preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "def preprocess_meteorological_data(file_path):\n",
    "    # Load data\n",
    "    met_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Combine DATE and MST into a single datetime column\n",
    "    # met_data['Timestamp'] = pd.to_datetime(met_data['DATE'] + ' ' + met_data['MST'])\n",
    "        # Combine DATE and MST into a single datetime column\n",
    "    met_data['Timestamp'] = pd.to_datetime(met_data['datetime'].astype(str), format='%Y%m%d%H%M%S')\n",
    "\n",
    "    # Sort data by Timestamp\n",
    "    met_data.sort_values('Timestamp', inplace=True)\n",
    "\n",
    "    # Handle missing values in input features using KNN imputation\n",
    "    input_features = ['Tower Dry Bulb Temp [deg C]', 'Tower RH [%]', 'Station Pressure [mBar]',\n",
    "                      'Avg Wind Speed @ 6ft [m/s]', 'Avg Wind Direction @ 6ft [deg from N]']\n",
    "\n",
    "    # Initialize KNN imputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # Fit and transform the input features\n",
    "    met_data_imputed = imputer.fit_transform(met_data[input_features])\n",
    "\n",
    "    # Update the DataFrame with imputed values\n",
    "    met_data[input_features] = met_data_imputed\n",
    "\n",
    "    # Handle missing values in the target variable separately\n",
    "    target_variable = 'Global CMP22 (vent/cor) [W/m^2]'\n",
    "\n",
    "    # Option 1: Interpolate missing target values\n",
    "    # met_data[target_variable].interpolate(method='time', inplace=True)\n",
    "\n",
    "    # Option 2: Drop rows with missing target values (uncomment if preferred)\n",
    "    met_data.dropna(subset=[target_variable], inplace=True)\n",
    "\n",
    "    # Rename columns for simplicity\n",
    "    met_data.rename(columns={\n",
    "        'Tower Dry Bulb Temp [deg C]': 'Temperature',\n",
    "        'Tower RH [%]': 'Humidity',\n",
    "        'Station Pressure [mBar]': 'Pressure',\n",
    "        'Avg Wind Speed @ 6ft [m/s]': 'Wind Speed',\n",
    "        'Avg Wind Direction @ 6ft [deg from N]': 'Wind Direction',\n",
    "        'Global CMP22 (vent/cor) [W/m^2]': 'Irradiance'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Feature scaling for input features\n",
    "    # Normalizes numerical features to a 0-1 range using Min-Max scaling.\n",
    "    scaler = MinMaxScaler()\n",
    "    met_data[['Temperature', 'Humidity', 'Pressure', 'Wind Speed']] = scaler.fit_transform(\n",
    "        met_data[['Temperature', 'Humidity', 'Pressure', 'Wind Speed']])\n",
    "    joblib.dump(scaler, 'scaler_y.pkl')\n",
    "\n",
    "    # Wind Direction encoding (convert degrees to sine and cosine components)\n",
    "    # Converts wind direction from degrees to sine and cosine components to handle its circular nature.\n",
    "    met_data['Wind Dir Sin'] = np.sin(np.deg2rad(met_data['Wind Direction']))\n",
    "    met_data['Wind Dir Cos'] = np.cos(np.deg2rad(met_data['Wind Direction']))\n",
    "    met_data.drop('Wind Direction', axis=1, inplace=True)\n",
    "\n",
    "    # Temporal features\n",
    "    # Extracts hour of the day and day of the year from the Timestamp.\n",
    "\t# Normalizes these features.\n",
    "    met_data['Hour'] = met_data['Timestamp'].dt.hour / 23.0  # Normalize Hour\n",
    "    met_data['DayOfYear'] = met_data['Timestamp'].dt.dayofyear / 365.0  # Normalize DayOfYear\n",
    "\n",
    "    # Prepare target variables (future irradiance)\n",
    "    target = 'Irradiance'\n",
    "    for minutes in [5, 15, 30, 60]:\n",
    "        met_data[f'Irradiance_{minutes}min_ahead'] = met_data[target].shift(-minutes)\n",
    "\n",
    "    # Drop rows with any remaining missing values (after shifting)\n",
    "    met_data.dropna(inplace=True)\n",
    "\n",
    "    # Reset index and return the processed DataFrame\n",
    "    return met_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/image_preprocessing.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_images(image_folder):\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_folder, '*.jpg')))\n",
    "    images = []\n",
    "    image_timestamps = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        # Extract timestamp from image filename\n",
    "        # Assuming filename format: image_YYYYMMDD_HHMM.jpg\n",
    "        filename = os.path.basename(path)\n",
    "        timestamp_str = filename.replace('.jpg', '')\n",
    "        timestamp = pd.to_datetime(timestamp_str, format='%Y%m%d%H%M%S')\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            continue  # Skip if the image is not readable\n",
    "        # Iterates over each image path, reads the image using OpenCV, and resizes it to 128x128 pixels\n",
    "\t    # Normalizes pixel values to the range [0, 1] by dividing by 255\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        img = img / 255.0  # Normalize pixel values\n",
    "        images.append(img)\n",
    "        image_timestamps.append(timestamp)\n",
    "\n",
    "    return images, image_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Irradiance</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Wind Dir Sin</th>\n",
       "      <th>Wind Dir Cos</th>\n",
       "      <th>Hour</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>Irradiance_5min_ahead</th>\n",
       "      <th>Irradiance_15min_ahead</th>\n",
       "      <th>Irradiance_30min_ahead</th>\n",
       "      <th>Irradiance_60min_ahead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230101000000</td>\n",
       "      <td>-0.899771</td>\n",
       "      <td>0.426223</td>\n",
       "      <td>0.616318</td>\n",
       "      <td>0.048901</td>\n",
       "      <td>0.250588</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>-0.763796</td>\n",
       "      <td>-0.645458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>-0.840211</td>\n",
       "      <td>-0.794020</td>\n",
       "      <td>-0.647894</td>\n",
       "      <td>-0.716989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230101000100</td>\n",
       "      <td>-0.876596</td>\n",
       "      <td>0.425231</td>\n",
       "      <td>0.618392</td>\n",
       "      <td>0.039946</td>\n",
       "      <td>0.250559</td>\n",
       "      <td>2023-01-01 00:01:00</td>\n",
       "      <td>-0.793353</td>\n",
       "      <td>-0.608761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>-0.889579</td>\n",
       "      <td>-0.809629</td>\n",
       "      <td>-0.621173</td>\n",
       "      <td>-0.776699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230101000200</td>\n",
       "      <td>-0.858901</td>\n",
       "      <td>0.423942</td>\n",
       "      <td>0.621088</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.250568</td>\n",
       "      <td>2023-01-01 00:02:00</td>\n",
       "      <td>-0.685818</td>\n",
       "      <td>-0.727773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>-0.926488</td>\n",
       "      <td>-0.844199</td>\n",
       "      <td>-0.645756</td>\n",
       "      <td>-0.811686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230101000300</td>\n",
       "      <td>-0.843903</td>\n",
       "      <td>0.423381</td>\n",
       "      <td>0.623266</td>\n",
       "      <td>0.063164</td>\n",
       "      <td>0.250575</td>\n",
       "      <td>2023-01-01 00:03:00</td>\n",
       "      <td>-0.799685</td>\n",
       "      <td>-0.600420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>-0.958692</td>\n",
       "      <td>-0.872226</td>\n",
       "      <td>-0.731193</td>\n",
       "      <td>-0.821406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230101000400</td>\n",
       "      <td>-0.859976</td>\n",
       "      <td>0.422571</td>\n",
       "      <td>0.625858</td>\n",
       "      <td>0.065147</td>\n",
       "      <td>0.250555</td>\n",
       "      <td>2023-01-01 00:04:00</td>\n",
       "      <td>-0.829038</td>\n",
       "      <td>-0.559193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>-0.977529</td>\n",
       "      <td>-0.854995</td>\n",
       "      <td>-0.716767</td>\n",
       "      <td>-0.846818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987775</th>\n",
       "      <td>20241116225500</td>\n",
       "      <td>-0.748687</td>\n",
       "      <td>0.433245</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>0.038445</td>\n",
       "      <td>0.254646</td>\n",
       "      <td>2024-11-16 22:55:00</td>\n",
       "      <td>-0.810042</td>\n",
       "      <td>-0.586372</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.879452</td>\n",
       "      <td>-0.776067</td>\n",
       "      <td>-0.733515</td>\n",
       "      <td>-0.750811</td>\n",
       "      <td>-0.600193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987776</th>\n",
       "      <td>20241116225600</td>\n",
       "      <td>-0.760332</td>\n",
       "      <td>0.433526</td>\n",
       "      <td>0.532219</td>\n",
       "      <td>0.039410</td>\n",
       "      <td>0.254645</td>\n",
       "      <td>2024-11-16 22:56:00</td>\n",
       "      <td>-0.811064</td>\n",
       "      <td>-0.584958</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.879452</td>\n",
       "      <td>-0.746704</td>\n",
       "      <td>-0.728514</td>\n",
       "      <td>-0.749872</td>\n",
       "      <td>-0.552789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987777</th>\n",
       "      <td>20241116225700</td>\n",
       "      <td>-0.771023</td>\n",
       "      <td>0.433890</td>\n",
       "      <td>0.533671</td>\n",
       "      <td>0.020643</td>\n",
       "      <td>0.254641</td>\n",
       "      <td>2024-11-16 22:57:00</td>\n",
       "      <td>-0.876307</td>\n",
       "      <td>-0.481754</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.879452</td>\n",
       "      <td>-0.755439</td>\n",
       "      <td>-0.744534</td>\n",
       "      <td>-0.756626</td>\n",
       "      <td>-0.577916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987778</th>\n",
       "      <td>20241116225800</td>\n",
       "      <td>-0.757686</td>\n",
       "      <td>0.434154</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>0.044611</td>\n",
       "      <td>0.254643</td>\n",
       "      <td>2024-11-16 22:58:00</td>\n",
       "      <td>-0.973579</td>\n",
       "      <td>-0.228351</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.879452</td>\n",
       "      <td>-0.763985</td>\n",
       "      <td>-0.745426</td>\n",
       "      <td>-0.747452</td>\n",
       "      <td>-0.572389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987779</th>\n",
       "      <td>20241116225900</td>\n",
       "      <td>-0.763630</td>\n",
       "      <td>0.434385</td>\n",
       "      <td>0.532323</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>0.254648</td>\n",
       "      <td>2024-11-16 22:59:00</td>\n",
       "      <td>-0.967709</td>\n",
       "      <td>-0.252069</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.879452</td>\n",
       "      <td>-0.742822</td>\n",
       "      <td>-0.744938</td>\n",
       "      <td>-0.742152</td>\n",
       "      <td>-0.569530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>987780 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  Irradiance  Temperature  Humidity  Wind Speed  \\\n",
       "0       20230101000000   -0.899771     0.426223  0.616318    0.048901   \n",
       "1       20230101000100   -0.876596     0.425231  0.618392    0.039946   \n",
       "2       20230101000200   -0.858901     0.423942  0.621088    0.063700   \n",
       "3       20230101000300   -0.843903     0.423381  0.623266    0.063164   \n",
       "4       20230101000400   -0.859976     0.422571  0.625858    0.065147   \n",
       "...                ...         ...          ...       ...         ...   \n",
       "987775  20241116225500   -0.748687     0.433245  0.537300    0.038445   \n",
       "987776  20241116225600   -0.760332     0.433526  0.532219    0.039410   \n",
       "987777  20241116225700   -0.771023     0.433890  0.533671    0.020643   \n",
       "987778  20241116225800   -0.757686     0.434154  0.531700    0.044611   \n",
       "987779  20241116225900   -0.763630     0.434385  0.532323    0.065684   \n",
       "\n",
       "        Pressure           Timestamp  Wind Dir Sin  Wind Dir Cos      Hour  \\\n",
       "0       0.250588 2023-01-01 00:00:00     -0.763796     -0.645458  0.000000   \n",
       "1       0.250559 2023-01-01 00:01:00     -0.793353     -0.608761  0.000000   \n",
       "2       0.250568 2023-01-01 00:02:00     -0.685818     -0.727773  0.000000   \n",
       "3       0.250575 2023-01-01 00:03:00     -0.799685     -0.600420  0.000000   \n",
       "4       0.250555 2023-01-01 00:04:00     -0.829038     -0.559193  0.000000   \n",
       "...          ...                 ...           ...           ...       ...   \n",
       "987775  0.254646 2024-11-16 22:55:00     -0.810042     -0.586372  0.956522   \n",
       "987776  0.254645 2024-11-16 22:56:00     -0.811064     -0.584958  0.956522   \n",
       "987777  0.254641 2024-11-16 22:57:00     -0.876307     -0.481754  0.956522   \n",
       "987778  0.254643 2024-11-16 22:58:00     -0.973579     -0.228351  0.956522   \n",
       "987779  0.254648 2024-11-16 22:59:00     -0.967709     -0.252069  0.956522   \n",
       "\n",
       "        DayOfYear  Irradiance_5min_ahead  Irradiance_15min_ahead  \\\n",
       "0        0.002740              -0.840211               -0.794020   \n",
       "1        0.002740              -0.889579               -0.809629   \n",
       "2        0.002740              -0.926488               -0.844199   \n",
       "3        0.002740              -0.958692               -0.872226   \n",
       "4        0.002740              -0.977529               -0.854995   \n",
       "...           ...                    ...                     ...   \n",
       "987775   0.879452              -0.776067               -0.733515   \n",
       "987776   0.879452              -0.746704               -0.728514   \n",
       "987777   0.879452              -0.755439               -0.744534   \n",
       "987778   0.879452              -0.763985               -0.745426   \n",
       "987779   0.879452              -0.742822               -0.744938   \n",
       "\n",
       "        Irradiance_30min_ahead  Irradiance_60min_ahead  \n",
       "0                    -0.647894               -0.716989  \n",
       "1                    -0.621173               -0.776699  \n",
       "2                    -0.645756               -0.811686  \n",
       "3                    -0.731193               -0.821406  \n",
       "4                    -0.716767               -0.846818  \n",
       "...                        ...                     ...  \n",
       "987775               -0.750811               -0.600193  \n",
       "987776               -0.749872               -0.552789  \n",
       "987777               -0.756626               -0.577916  \n",
       "987778               -0.747452               -0.572389  \n",
       "987779               -0.742152               -0.569530  \n",
       "\n",
       "[987780 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met_data = preprocess_meteorological_data('../combined_data.csv')\n",
    "met_data\n",
    "# images, image_timestamps = preprocess_images('data/images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47416\n",
      "47416\n"
     ]
    }
   ],
   "source": [
    "images, image_timestamps = preprocess_images('../training_images/')\n",
    "print(len(images))\n",
    "print(len(image_timestamps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.07843137 0.0745098  0.08627451]\n",
      "  [0.09019608 0.09019608 0.09019608]\n",
      "  [0.0627451  0.0627451  0.0627451 ]\n",
      "  ...\n",
      "  [0.09803922 0.09803922 0.09803922]\n",
      "  [0.0745098  0.0745098  0.0745098 ]\n",
      "  [0.08627451 0.08627451 0.08627451]]\n",
      "\n",
      " [[0.92941176 0.92941176 0.92941176]\n",
      "  [0.92941176 0.92941176 0.92941176]\n",
      "  [0.89019608 0.89019608 0.89019608]\n",
      "  ...\n",
      "  [0.0627451  0.0627451  0.0627451 ]\n",
      "  [0.05882353 0.05882353 0.05882353]\n",
      "  [0.05882353 0.05882353 0.05882353]]\n",
      "\n",
      " [[0.30980392 0.30980392 0.30980392]\n",
      "  [0.30588235 0.30588235 0.30588235]\n",
      "  [0.27058824 0.27058824 0.27058824]\n",
      "  ...\n",
      "  [0.0745098  0.0745098  0.0745098 ]\n",
      "  [0.0745098  0.0745098  0.0745098 ]\n",
      "  [0.07058824 0.07058824 0.07058824]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.07058824 0.07058824 0.07058824]\n",
      "  [0.07058824 0.07058824 0.07058824]\n",
      "  [0.07058824 0.07058824 0.07058824]\n",
      "  ...\n",
      "  [0.08235294 0.08235294 0.08235294]\n",
      "  [0.07843137 0.07843137 0.07843137]\n",
      "  [0.0627451  0.0627451  0.0627451 ]]\n",
      "\n",
      " [[0.07843137 0.07843137 0.07843137]\n",
      "  [0.06666667 0.06666667 0.06666667]\n",
      "  [0.0745098  0.0745098  0.0745098 ]\n",
      "  ...\n",
      "  [0.07058824 0.07058824 0.07058824]\n",
      "  [0.08235294 0.08235294 0.08235294]\n",
      "  [0.0627451  0.0627451  0.0627451 ]]\n",
      "\n",
      " [[0.07843137 0.07843137 0.07843137]\n",
      "  [0.09019608 0.09019608 0.09019608]\n",
      "  [0.07843137 0.07843137 0.07843137]\n",
      "  ...\n",
      "  [0.06666667 0.06666667 0.06666667]\n",
      "  [0.07058824 0.07058824 0.07058824]\n",
      "  [0.06666667 0.06666667 0.06666667]]]\n"
     ]
    }
   ],
   "source": [
    "print(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/train_model.py\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from models.hybrid import create_hybrid_model\n",
    "import pandas as pd\n",
    "\n",
    "def train_hybrid_model(met_data, images, image_timestamps):\n",
    "    # Prepare data\n",
    "    sequence_length = 60  # Number of past minutes to consider\n",
    "    features = ['Temperature', 'Humidity', 'Pressure', 'Wind Speed', 'Wind Dir Sin', 'Wind Dir Cos',\n",
    "                'Hour', 'DayOfYear']\n",
    "\n",
    "    # Align images with meteorological data\n",
    "    met_data = align_data_with_images(met_data, images, image_timestamps)\n",
    "\n",
    "    # Extract features and targets\n",
    "    X_num = met_data[features].values\n",
    "    y = met_data[[f'Irradiance_{minutes}min_ahead' for minutes in [5, 15, 30, 60]]].values\n",
    "    X_img = np.array(met_data['Image'].tolist())\n",
    "\n",
    "    # Create sequences\n",
    "    def create_sequences(X_num, X_img, y, seq_length):\n",
    "        X_num_seq, X_img_seq, y_seq = [], [], []\n",
    "        for i in range(len(X_num) - seq_length):\n",
    "            X_num_seq.append(X_num[i:i+seq_length])\n",
    "            X_img_seq.append(X_img[i+seq_length-1])  # Use image at the last timestamp in the sequence\n",
    "            y_seq.append(y[i+seq_length-1])\n",
    "        return np.array(X_num_seq), np.array(X_img_seq), np.array(y_seq)\n",
    "\n",
    "    X_num_seq, X_img_seq, y_seq = create_sequences(X_num, X_img, y, sequence_length)\n",
    "\n",
    "    # Train-test split\n",
    "    split_index = int(0.8 * len(X_num_seq))\n",
    "    X_num_train, X_num_test = X_num_seq[:split_index], X_num_seq[split_index:]\n",
    "    X_img_train, X_img_test = X_img_seq[:split_index], X_img_seq[split_index:]\n",
    "    y_train, y_test = y_seq[:split_index], y_seq[split_index:]\n",
    "\n",
    "    # Create model\n",
    "    num_features = X_num_train.shape[2]\n",
    "    model = create_hybrid_model(sequence_length, num_features)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        [X_img_train, X_num_train],\n",
    "        y_train,\n",
    "        validation_data=([X_img_test, X_num_test], y_test),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def align_data_with_images(met_data, images, image_timestamps):\n",
    "    # Ensure all inputs are lists/arrays\n",
    "    images = list(images)\n",
    "    image_timestamps = list(image_timestamps)\n",
    "\n",
    "    # Create a DataFrame for image timestamps\n",
    "    image_df = pd.DataFrame({\n",
    "        'Timestamp': image_timestamps, \n",
    "        'Image': [tuple(img.flatten()) for img in images]  # Convert images to tuples\n",
    "    })\n",
    "    image_df['Timestamp'] = pd.to_datetime(image_df['Timestamp'])\n",
    "\n",
    "    # Merge meteorological data with images\n",
    "    met_data['Timestamp'] = pd.to_datetime(met_data['Timestamp'])\n",
    "    merged_data = pd.merge_asof(\n",
    "        met_data.sort_values('Timestamp'),\n",
    "        image_df.sort_values('Timestamp'),\n",
    "        on='Timestamp',\n",
    "        direction='backward'\n",
    "    )\n",
    "\n",
    "    # If any images are missing, create a Series with the first image\n",
    "    if merged_data['Image'].isnull().any():\n",
    "        first_image_tuple = tuple(images[0].flatten())\n",
    "        merged_data['Image'] = merged_data['Image'].fillna(first_image_tuple)\n",
    "\n",
    "    # Convert tuples back to numpy arrays\n",
    "    merged_data['Image'] = merged_data['Image'].apply(lambda x: np.array(x).reshape(images[0].shape))\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_hybrid_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmet_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_timestamps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 16\u001b[0m, in \u001b[0;36mtrain_hybrid_model\u001b[1;34m(met_data, images, image_timestamps)\u001b[0m\n\u001b[0;32m     12\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHumidity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPressure\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind Speed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind Dir Sin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind Dir Cos\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDayOfYear\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Align images with meteorological data\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m met_data \u001b[38;5;241m=\u001b[39m \u001b[43malign_data_with_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmet_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_timestamps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Extract features and targets\u001b[39;00m\n\u001b[0;32m     19\u001b[0m X_num \u001b[38;5;241m=\u001b[39m met_data[features]\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[1;32mIn[31], line 71\u001b[0m, in \u001b[0;36malign_data_with_images\u001b[1;34m(met_data, images, image_timestamps)\u001b[0m\n\u001b[0;32m     66\u001b[0m image_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image_timestamps)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for image timestamps\u001b[39;00m\n\u001b[0;32m     69\u001b[0m image_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: image_timestamps, \n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28mtuple\u001b[39m(img\u001b[38;5;241m.\u001b[39mflatten()) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]  \u001b[38;5;66;03m# Convert images to tuples\u001b[39;00m\n\u001b[0;32m     72\u001b[0m })\n\u001b[0;32m     73\u001b[0m image_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(image_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Merge meteorological data with images\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     66\u001b[0m image_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image_timestamps)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for image timestamps\u001b[39;00m\n\u001b[0;32m     69\u001b[0m image_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: image_timestamps, \n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]  \u001b[38;5;66;03m# Convert images to tuples\u001b[39;00m\n\u001b[0;32m     72\u001b[0m })\n\u001b[0;32m     73\u001b[0m image_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(image_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Merge meteorological data with images\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, history = train_hybrid_model(met_data, images, image_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, met_data, images):\n",
    "    # Prepare data (similar to training data preparation)\n",
    "    sequence_length = 60\n",
    "    features = ['Temperature', 'Humidity', 'Pressure', 'Wind Speed', 'Wind Dir Sin', 'Wind Dir Cos',\n",
    "                'Hour', 'DayOfYear']  # Exclude direct current irradiance as a feature\n",
    "\n",
    "    X_num = met_data[features].values\n",
    "    y = met_data[[f'Irradiance_{minutes}min_ahead' for minutes in [5, 15, 30, 60]]].values\n",
    "    X_img = np.array(images)\n",
    "\n",
    "    # Create sequences\n",
    "    def create_sequences(X_num, X_img, y, seq_length):\n",
    "        X_num_seq, X_img_seq, y_seq = [], [], []\n",
    "        for i in range(len(X_num) - seq_length):\n",
    "            X_num_seq.append(X_num[i:i+seq_length])\n",
    "            X_img_seq.append(X_img[i+seq_length-1])\n",
    "            y_seq.append(y[i+seq_length-1])\n",
    "        return np.array(X_num_seq), np.array(X_img_seq), np.array(y_seq)\n",
    "\n",
    "    X_num_seq, X_img_seq, y_seq = create_sequences(X_num, X_img, y, sequence_length)\n",
    "\n",
    "    # Use the last 20% for evaluation\n",
    "    split_index = int(0.8 * len(X_num_seq))\n",
    "    X_num_test = X_num_seq[split_index:]\n",
    "    X_img_test = X_img_seq[split_index:]\n",
    "    y_test = y_seq[split_index:]\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict([X_img_test, X_num_test])\n",
    "\n",
    "    # Calculate RMSE and MAE for each horizon\n",
    "    horizons = [5, 15, 30, 60]\n",
    "    for i, minutes in enumerate(horizons):\n",
    "        rmse = np.sqrt(mean_squared_error(y_test[:, i], y_pred[:, i]))\n",
    "        mae = mean_absolute_error(y_test[:, i], y_pred[:, i])\n",
    "        print(f'{minutes}-Minute Ahead Prediction - RMSE: {rmse:.2f}, MAE: {mae:.2f}')\n",
    "\n",
    "        # Plot actual vs predicted\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(y_test[:, i], label='Actual')\n",
    "        plt.plot(y_pred[:, i], label='Predicted')\n",
    "        plt.title(f'{minutes}-Minute Ahead Prediction')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('Irradiance (W/m^2)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Explicitly allow GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(gpu)\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
